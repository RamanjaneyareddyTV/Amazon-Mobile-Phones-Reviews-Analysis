{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv('D:/ML/Amazon Mobile Phones Reviews Analysis/Dataset/cell_phone_review.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>vote</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>10-02-2010</td>\n",
       "      <td>Tech support is the worst</td>\n",
       "      <td>1265760000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>24-10-2016</td>\n",
       "      <td>Screws were missing from the bracket and beaut...</td>\n",
       "      <td>Spend a little more and get much better.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10-07-2017</td>\n",
       "      <td>Trouble connecting and staying connected via b...</td>\n",
       "      <td>1499644800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>02-05-2013</td>\n",
       "      <td>I purchased this unit for our RV to replace an...</td>\n",
       "      <td>Receiver Offers a Lot of Flexibility &amp; Complexity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>04-01-2013</td>\n",
       "      <td>It works.  Nuff said but the review requires 1...</td>\n",
       "      <td>It's a cable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>03-12-2013</td>\n",
       "      <td>A great lens for the money.</td>\n",
       "      <td>1386028800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>23-01-2015</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>1421971200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>21-12-2017</td>\n",
       "      <td>Fast and great when it works...</td>\n",
       "      <td>1513814400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>04-11-2015</td>\n",
       "      <td>XLNT!</td>\n",
       "      <td>XLNT!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>19-02-2015</td>\n",
       "      <td>Not sure if it's a bad camera or design, but t...</td>\n",
       "      <td>Infrared failure?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>189 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     overall  vote  reviewTime  \\\n",
       "0          2     0  10-02-2010   \n",
       "1          2     0  24-10-2016   \n",
       "2          1     0  10-07-2017   \n",
       "3          4     5  02-05-2013   \n",
       "4          3     0  04-01-2013   \n",
       "..       ...   ...         ...   \n",
       "184        4     0  03-12-2013   \n",
       "185        5     0  23-01-2015   \n",
       "186        1     0  21-12-2017   \n",
       "187        5     0  04-11-2015   \n",
       "188        2     3  19-02-2015   \n",
       "\n",
       "                                            reviewText  \\\n",
       "0                            Tech support is the worst   \n",
       "1    Screws were missing from the bracket and beaut...   \n",
       "2    Trouble connecting and staying connected via b...   \n",
       "3    I purchased this unit for our RV to replace an...   \n",
       "4    It works.  Nuff said but the review requires 1...   \n",
       "..                                                 ...   \n",
       "184                        A great lens for the money.   \n",
       "185                                         Five Stars   \n",
       "186                    Fast and great when it works...   \n",
       "187                                              XLNT!   \n",
       "188  Not sure if it's a bad camera or design, but t...   \n",
       "\n",
       "                                               summary  \n",
       "0                                           1265760000  \n",
       "1             Spend a little more and get much better.  \n",
       "2                                           1499644800  \n",
       "3    Receiver Offers a Lot of Flexibility & Complexity  \n",
       "4                                         It's a cable  \n",
       "..                                                 ...  \n",
       "184                                         1386028800  \n",
       "185                                         1421971200  \n",
       "186                                         1513814400  \n",
       "187                                              XLNT!  \n",
       "188                                  Infrared failure?  \n",
       "\n",
       "[189 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sklearn\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "from sklearn import tree\n",
    "\n",
    "\n",
    "import pickle\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import string\n",
    "import warnings\n",
    "import nltk\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "from nltk import tokenize,WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import wordnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['review','sentiment']]=df[['reviewText','overall']]\n",
    "\n",
    "df=df[['review', 'sentiment']]\n",
    "\n",
    "df['sentiment']=df['sentiment'].replace([1,2,3,4,5],[0,0,0,1,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tech support is the worst</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Screws were missing from the bracket and beaut...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Trouble connecting and staying connected via b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I purchased this unit for our RV to replace an...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It works.  Nuff said but the review requires 1...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>A great lens for the money.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>Five Stars</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>Fast and great when it works...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>XLNT!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>Not sure if it's a bad camera or design, but t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>189 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                review  sentiment\n",
       "0                            Tech support is the worst          0\n",
       "1    Screws were missing from the bracket and beaut...          0\n",
       "2    Trouble connecting and staying connected via b...          0\n",
       "3    I purchased this unit for our RV to replace an...          1\n",
       "4    It works.  Nuff said but the review requires 1...          0\n",
       "..                                                 ...        ...\n",
       "184                        A great lens for the money.          1\n",
       "185                                         Five Stars          1\n",
       "186                    Fast and great when it works...          0\n",
       "187                                              XLNT!          1\n",
       "188  Not sure if it's a bad camera or design, but t...          0\n",
       "\n",
       "[189 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sentiment.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "df['sentiment'].value_counts().plot(kind='bar');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df=df.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(189, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Raman/nltk_data'\n    - 'C:\\\\Users\\\\Raman\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\nltk_data'\n    - 'C:\\\\Users\\\\Raman\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Raman\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Raman\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Raman/nltk_data'\n    - 'C:\\\\Users\\\\Raman\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\nltk_data'\n    - 'C:\\\\Users\\\\Raman\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Raman\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Raman\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11256/273457207.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mstopwords1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mPorterStemmer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mlemmatizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    119\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m         \u001b[1;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;31m# __class__ to something new:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[1;31m# Load the corpus.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m                 \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{self.subdir}/{self.__name}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"*\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Raman/nltk_data'\n    - 'C:\\\\Users\\\\Raman\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\nltk_data'\n    - 'C:\\\\Users\\\\Raman\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Raman\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Raman\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "\n",
    "stopwords1 = list(stopwords.words('english'))+list(punctuation) \n",
    "\n",
    "ps=PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "   \n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:         \n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def lemmatize_sentence(sentence):  \n",
    "   \n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence)) \n",
    "   \n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "\n",
    "   \n",
    "    lemmatized_sentence = []      \n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            lemmatized_sentence.append(word)\n",
    "       \n",
    "        else:       \n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "\n",
    "   \n",
    "    return \" \".join(lemmatized_sentence)\n",
    "\n",
    "print(lemmatize_sentence(\" i have been working on my skills \"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(lemmatize_sentence(df['review'][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(a):\n",
    "    return  ' '.join([i.lower() for i in tokenize.word_tokenize(a) if i.lower() not in stopwords1])\n",
    " \n",
    "   \n",
    "    if len(ls)>2:\n",
    "        val= ' '.join(ls)\n",
    "        return val\n",
    "    else :\n",
    "        return None\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_lemma(a):\n",
    "    val= ' '.join([lemmatizer.lemmatize(i.lower(),pos = 'v') for i in tokenize.word_tokenize(a) if i.lower() not in stopwords1])\n",
    "    return val\n",
    "\n",
    "\n",
    "\n",
    "def clean_text_stem(a):   \n",
    "    val= ' '.join([ps.stem(i.lower()) for i in tokenize.word_tokenize(a) if i.lower() not in stopwords1])\n",
    "    return val\n",
    "\n",
    "df['clean_txt'] = df['review'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review']=df['clean_txt']\n",
    "\n",
    "df.dropna()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df[['review', 'sentiment']]\n",
    "\n",
    "df=df1\n",
    "\n",
    "def simple_split(df,y,l,sm=0.8):\n",
    "    if sm>0 and sm<1.0:\n",
    "        n=int(sm*l)\n",
    "    else:\n",
    "        n=int(sm)\n",
    "    X_train=df[:n].copy()\n",
    "    X_test=df[n:].copy()\n",
    "    y_train=y[:n].copy()\n",
    "    y_test=y[n:].copy()\n",
    "    return X_train,X_test,y_train,y_test\n",
    "        \n",
    "\n",
    "v=CountVectorizer()\n",
    "\n",
    "X_train,X_test,y_train,y_test=simple_split(df.review, df.sentiment, len(df))\n",
    "\n",
    "print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)\n",
    "\n",
    "print(np.bincount(y_train))\n",
    "\n",
    "print(np.bincount(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Logistic regression\")\n",
    "clf2=LogisticRegression(solver=\"lbfgs\")\n",
    "model=Pipeline([('vectorizer',v),('classifier',clf2)])\n",
    "model.fit(X_train, y_train)\n",
    "p=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(p,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(p, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Decission Tree Classifier\")\n",
    "c=tree.DecisionTreeClassifier()\n",
    "model2=Pipeline([('vectorizer',v),('classifier',c)])\n",
    "\n",
    "model2.fit(X_train, y_train)\n",
    "\n",
    "p=model2.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(p,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(p, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier as rfc\n",
    "print(\"Random Forest Classifier\")\n",
    "r=rfc()\n",
    "model2=Pipeline([('vectorizer',v),('classifier',r)])\n",
    "model2.fit(X_train, y_train)\n",
    "p=model2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(p,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(p, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc=SVC()\n",
    "model2=Pipeline([('vectorizer',v),('classifier',svc)])\n",
    "model2.fit(X_train, y_train)\n",
    "p=model2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(p,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(p, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "estimator = []\n",
    "estimator.append(('LR', \n",
    "                  LogisticRegression(solver ='lbfgs', \n",
    "                                     multi_class ='multinomial', \n",
    "                                     max_iter = 200)))\n",
    "estimator.append(('SVC', SVC(gamma ='auto', probability = True)))\n",
    "estimator.append(('DTC', DecisionTreeClassifier()))\n",
    "  \n",
    "c=VotingClassifier(estimators=estimator,voting='hard')\n",
    "\n",
    "model2=Pipeline([('vectorizer',v),('classifier',c)])\n",
    "\n",
    "model2.fit(X_train, y_train)\n",
    "\n",
    "p=model2.predict(X_test)\n",
    "print(\"Voting Classifier\")\n",
    "print(confusion_matrix(p,y_test))\n",
    "print(accuracy_score(p, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "estimator = []\n",
    "estimator.append(('LR', \n",
    "                  LogisticRegression(solver ='lbfgs', \n",
    "                                     multi_class ='multinomial', \n",
    "                                     max_iter = 200)))\n",
    "estimator.append(('SVC', SVC(gamma ='auto', probability = True)))\n",
    "estimator.append(('DTC', DecisionTreeClassifier()))\n",
    "  \n",
    "c=VotingClassifier(estimators=estimator,voting='soft')\n",
    "\n",
    "model2=Pipeline([('vectorizer',v),('classifier',c)])\n",
    "\n",
    "model2.fit(X_train, y_train)\n",
    "\n",
    "p=model2.predict(X_test)\n",
    "print(\"Voting Classifier\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(accuracy_score(p, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(p,y_test))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
